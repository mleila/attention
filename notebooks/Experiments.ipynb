{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "muslim-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "harmful-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "\n",
    "from attention import models\n",
    "from attention import utils\n",
    "from attention.vectorizer import Vectorizer\n",
    "from attention.constants import ENGLISH, FRENCH, SEQ_SIZE, DECODER_INPUT, ENCODER_INPUT, SOS_token\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SOURCE_DIR = Path('../')\n",
    "DATA_DIR = SOURCE_DIR / 'data'\n",
    "translation_fp = DATA_DIR / 'eng-fra.txt'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-egyptian",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "opening-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer code\n",
    "from attention.data import load_sentences_dataframe, assign_rows_to_split, TranslationDataset, generate_batches\n",
    "from attention.vectorizer import Vectorizer\n",
    "\n",
    "df = load_sentences_dataframe(translation_fp)\n",
    "df = assign_rows_to_split(df, train_ratio=0.9, valid_ratio=0.05, test_ratio=0.05)\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "df[ENGLISH] = df[ENGLISH].str.lower()\n",
    "df = df[df[ENGLISH].str.startswith(eng_prefixes)]\n",
    "\n",
    "dataset = TranslationDataset.from_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-geography",
   "metadata": {},
   "source": [
    "# Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "wound-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model params\n",
    "from attention.models import EncoderRNN, DecoderRNN\n",
    "from attention.embeddings import create_spacy_embeddings\n",
    "\n",
    "hidden_size = 256\n",
    "embedding_size = 300\n",
    "english_vocab_size = len(dataset.vectorizer.english_vocab)\n",
    "french_vocab_size = len(dataset.vectorizer.french_vocab)\n",
    "\n",
    "use_pretrained_embeddings = False\n",
    "english_embedding_matrix, french_embedding_matrix = create_spacy_embeddings(dataset.vectorizer)\n",
    "\n",
    "# use pretrained embeddings?\n",
    "if use_pretrained_embeddings:\n",
    "    encoder = EncoderRNN(english_vocab_size, hidden_size, embedding_matrix=english_embedding_matrix)\n",
    "    decoder = DecoderRNN(hidden_size, french_vocab_size, embedding_matrix=french_embedding_matrix)\n",
    "else:\n",
    "    encoder = EncoderRNN(english_vocab_size, hidden_size, embedding_size)\n",
    "    decoder = DecoderRNN(hidden_size, french_vocab_size, embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-suspect",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hundred-holly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at 1 epoch is 3.35\n",
      "loss at 2 epoch is 2.94\n",
      "loss at 3 epoch is 2.49\n",
      "loss at 4 epoch is 2.60\n",
      "loss at 5 epoch is 2.74\n",
      "loss at 6 epoch is 2.30\n",
      "loss at 7 epoch is 2.49\n",
      "loss at 8 epoch is 2.43\n",
      "loss at 9 epoch is 2.27\n",
      "loss at 10 epoch is 2.43\n",
      "loss at 11 epoch is 2.22\n",
      "loss at 12 epoch is 2.40\n",
      "loss at 13 epoch is 2.07\n",
      "loss at 14 epoch is 2.27\n",
      "loss at 15 epoch is 2.25\n"
     ]
    }
   ],
   "source": [
    "from attention.train import train_simpleRNN_batch\n",
    "\n",
    "# training params\n",
    "nb_epochs = 15\n",
    "learning_rate = 0.001\n",
    "ignore_index = dataset.vectorizer.english_vocab.lookup_token(dataset.vectorizer.english_vocab.pad)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "batch_size = 32\n",
    "\n",
    "# create optimizers\n",
    "encoder_optim = torch.optim.Adam(encoder.parameters(), learning_rate)\n",
    "decoder_optim = torch.optim.Adam(decoder.parameters(), learning_rate)\n",
    "\n",
    "# run epochs\n",
    "for epoch in range(nb_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch in generate_batches(dataset, batch_size=batch_size):\n",
    "        input_batch, target_batch = batch[ENCODER_INPUT], batch[DECODER_INPUT]\n",
    "        loss = train_simpleRNN_batch(\n",
    "            input_batch, \n",
    "            target_batch, \n",
    "            encoder,\n",
    "            decoder,\n",
    "            encoder_optim,\n",
    "            decoder_optim,\n",
    "            criterion,\n",
    "            device,\n",
    "            use_teacher_forcing=True\n",
    "        )\n",
    "        epoch_losses.append(loss)\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    print(f'loss at {epoch+1} epoch is {avg_loss:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-skating",
   "metadata": {},
   "source": [
    "# Inference (Translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "advanced-bridges",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am only warming up now. => pour l instant je m entraine seulement .\n",
      "you are both in the wrong. => vous etes en train de s .\n",
      "he is said to have died => on le dit qu il n est\n",
      "i am bored out of my mind. => je suis en train de ma parole .\n",
      "i am going to stay here for a couple of days. => je suis etonnee que vous que vous etes la\n",
      "they are out shopping. => elles sont sorties faire les enfants\n",
      "i am afraid he will make a mistake. => je crains qu il commette commette une erreur .\n",
      "we are worried about you. => ce n est pas infirmiere mais docteur\n",
      "he likes to go to work => je suis interesse par ton audace .\n",
      "he is not at all foolish => il n est pas vraiment en difficulte\n"
     ]
    }
   ],
   "source": [
    "from attention.utils import translate_simple_rnn\n",
    "\n",
    "sentences = [\n",
    "    \"i am only warming up now.\",\n",
    "    \"you are both in the wrong.\",\n",
    "    \"he is said to have died\",\n",
    "    \"i am bored out of my mind.\",\n",
    "    \"i am going to stay here for a couple of days.\",\n",
    "    \"they are out shopping.\",\n",
    "    \"i am afraid he will make a mistake.\",\n",
    "    \"we are worried about you.\",\n",
    "    \"he likes to go to work\",\n",
    "    \"he is not at all foolish\"\n",
    "]\n",
    "for sent in sentences:\n",
    "    translation = translate_simple_rnn(sent, encoder, decoder, dataset.vectorizer, device)\n",
    "    print(sent, '=>', translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "narrow-orleans",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'je suis interesse par l'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_simple_rnn('i am interested.', encoder, decoder, dataset.vectorizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "mental-power",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40263</th>\n",
       "      <td>they are great friends.</td>\n",
       "      <td>Elles sont de grandes amies.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68831</th>\n",
       "      <td>he is our teacher of english.</td>\n",
       "      <td>Il est notre professeur d'anglais.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41358</th>\n",
       "      <td>you are hearing things.</td>\n",
       "      <td>Vous entendez des choses.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5278</th>\n",
       "      <td>she is french.</td>\n",
       "      <td>Elle est française.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46335</th>\n",
       "      <td>you are blinded by love.</td>\n",
       "      <td>Vous êtes aveuglé par l'amour.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62648</th>\n",
       "      <td>you are taller than she is.</td>\n",
       "      <td>Tu es plus grand qu'elle.</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23459</th>\n",
       "      <td>i am like my mother.</td>\n",
       "      <td>Je suis comme ma mère.</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107563</th>\n",
       "      <td>i am much obliged to you for your help.</td>\n",
       "      <td>J'apprécie beaucoup ton aide.</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29833</th>\n",
       "      <td>she is angry with me.</td>\n",
       "      <td>Elle est en colère après moi.</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40271</th>\n",
       "      <td>they are very cheerful.</td>\n",
       "      <td>Elles sont fort joyeuses.</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3203 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        english  \\\n",
       "40263                   they are great friends.   \n",
       "68831             he is our teacher of english.   \n",
       "41358                   you are hearing things.   \n",
       "5278                             she is french.   \n",
       "46335                  you are blinded by love.   \n",
       "...                                         ...   \n",
       "62648               you are taller than she is.   \n",
       "23459                      i am like my mother.   \n",
       "107563  i am much obliged to you for your help.   \n",
       "29833                     she is angry with me.   \n",
       "40271                   they are very cheerful.   \n",
       "\n",
       "                                    french  split  \n",
       "40263         Elles sont de grandes amies.  train  \n",
       "68831   Il est notre professeur d'anglais.  train  \n",
       "41358            Vous entendez des choses.  train  \n",
       "5278                   Elle est française.  train  \n",
       "46335       Vous êtes aveuglé par l'amour.  train  \n",
       "...                                    ...    ...  \n",
       "62648            Tu es plus grand qu'elle.   test  \n",
       "23459               Je suis comme ma mère.   test  \n",
       "107563       J'apprécie beaucoup ton aide.   test  \n",
       "29833        Elle est en colère après moi.   test  \n",
       "40271            Elles sont fort joyeuses.   test  \n",
       "\n",
       "[3203 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bibliographic-literacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3203"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-depression",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention_env",
   "language": "python",
   "name": "attention_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
