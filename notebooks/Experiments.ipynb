{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "gross-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "civic-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "\n",
    "from attention import models\n",
    "from attention import utils\n",
    "from attention.vectorizer import Vectorizer\n",
    "from attention.constants import ENGLISH, FRENCH, SEQ_SIZE, DECODER_INPUT, ENCODER_INPUT\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SOURCE_DIR = Path('../')\n",
    "DATA_DIR = SOURCE_DIR / 'data'\n",
    "translation_fp = DATA_DIR / 'eng-fra.txt'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-elephant",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "graduate-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer code\n",
    "from attention.data import load_sentences_dataframe, assign_rows_to_split, TranslationDataset, generate_batches\n",
    "from attention.vectorizer import Vectorizer\n",
    "\n",
    "df = load_sentences_dataframe(translation_fp)\n",
    "df = assign_rows_to_split(df, train_ratio=0.9, valid_ratio=0.05, test_ratio=0.05)\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "df[ENGLISH] = df[ENGLISH].str.lower()\n",
    "df = df[df[ENGLISH].str.startswith(eng_prefixes)]\n",
    "\n",
    "dataset = TranslationDataset.from_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-motivation",
   "metadata": {},
   "source": [
    "# Source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "medium-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_size, embedding_matrix=None):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # embedding layer\n",
    "        if embedding_matrix is None:\n",
    "            self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        else:\n",
    "            embedding_size = embedding_matrix.shape[-1]\n",
    "            self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "            \n",
    "        \n",
    "        # rnn layer\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, embedding_size, embedding_matrix=None):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # embedding layer\n",
    "        if embedding_matrix is None:\n",
    "            self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        else:\n",
    "            embedding_size = embedding_matrix.shape[-1]\n",
    "            self.embedding = nn.Embedding.from_pretrained(embedding_matrix, )\n",
    "            \n",
    "        \n",
    "        # rnn layer\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = torch.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "    \n",
    "class SimpleRnn(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(SimpleRnn, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, input_sentence, output_sentence):\n",
    "        hidden = self.encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(1, SEQ_SIZE, self.encoder.hidden_size)\n",
    "        \n",
    "        for idx in range(SEQ_SIZE-1):\n",
    "            token = input_sentence[idx]\n",
    "            output, hidden = self.encoder(token, hidden)\n",
    "            encoder_outputs[0, idx] = output[-1]\n",
    "\n",
    "        decoder_outputs = torch.zeros(1, SEQ_SIZE-1, self.decoder.output_size)\n",
    "        for idx in range(SEQ_SIZE-1):\n",
    "            token = output_sentence[idx]\n",
    "            output, hidden = self.decoder(token, hidden)\n",
    "            decoder_outputs[0, idx] = output[-1]\n",
    "            \n",
    "        return decoder_outputs, hidden\n",
    "    \n",
    "def train_one_sent(sent, translation, model, optim):\n",
    "    optim.zero_grad()\n",
    "    decoder_outputs, hidden = model(sent, translation)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(decoder_outputs.squeeze(0), translation_sentence[1:])\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# translate\n",
    "def translate(sentence, model, vectorizer):\n",
    "    vectorized = vectorizer.vectorize_sentence(sentence, language=ENGLISH)\n",
    "    french_vocab = vectorizer.french_vocab\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    for idx in range(SEQ_SIZE):\n",
    "        token = vectorized[idx]\n",
    "        _, encoder_hidden = encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
    "\n",
    "    decoder_outputs = torch.zeros(1, SEQ_SIZE-1, decoder.output_size)\n",
    "\n",
    "    for idx in range(SEQ_SIZE-1):\n",
    "        token = vectorized[idx]\n",
    "        output, hidden =  decoder(token, encoder_hidden)\n",
    "        decoder_outputs[0, idx] = output[-1]\n",
    "\n",
    "    \n",
    "    indices = torch.argmax(decoder_outputs, dim=-1).squeeze(0)\n",
    "    words = [french_vocab.lookup_index(idx) for idx in indices]\n",
    "    print(' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-trader",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "august-nursery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at 1 epoch is 3.87\n",
      "loss at 2 epoch is 0.44\n",
      "loss at 3 epoch is 1.67\n",
      "loss at 4 epoch is 0.73\n",
      "loss at 5 epoch is 0.91\n",
      "loss at 6 epoch is 0.82\n",
      "loss at 7 epoch is 0.53\n",
      "loss at 8 epoch is 0.31\n",
      "loss at 9 epoch is 0.53\n",
      "loss at 10 epoch is 0.14\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "hidden_size = 256\n",
    "vectorizer = dataset.vectorizer\n",
    "english_vocab_size = len(vectorizer.english_vocab)\n",
    "french_vocab_size = len(vectorizer.french_vocab)\n",
    "\n",
    "encoder = EncoderRNN(english_vocab_size, hidden_size)\n",
    "decoder = DecoderRNN(hidden_size, french_vocab_size)\n",
    "model = SimpleRnn(encoder, decoder)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), 0.001)\n",
    "\n",
    "\n",
    "nb_epochs = 10\n",
    "for epoch in range(nb_epochs):\n",
    "    for batch in generate_batches(dataset, batch_size=1):\n",
    "        single_sentence, translation_sentence = batch[ENCODER_INPUT], batch[DECODER_INPUT]\n",
    "        single_sentence = single_sentence.squeeze(0)\n",
    "        translation_sentence = translation_sentence.squeeze(0)\n",
    "        loss = train_one_sent(\n",
    "            single_sentence, \n",
    "            translation_sentence, \n",
    "            model,\n",
    "            optim\n",
    "        )\n",
    "    print(f'loss at {epoch+1} epoch is {loss:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "quiet-manitoba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il ce de il il il de <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"he went to school by bus\"\n",
    "translate(sentence, model, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-sugar",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "electronic-screening",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext.util\n",
    "\n",
    "fasttext.util.download_model('en', if_exists='ignore')\n",
    "fasttext.util.download_model('fr', if_exists='ignore')\n",
    "\n",
    "ft_english = fasttext.load_model('cc.en.300.bin')\n",
    "ft_french = fasttext.load_model('cc.fr.300.bin')\n",
    "\n",
    "vectorizer = dataset.vectorizer\n",
    "english_embedding_matrix = vectorizer.build_embedding_matrix_from_fasttext(ft_english, lang=ENGLISH)\n",
    "french_embedding_matrix = vectorizer.build_embedding_matrix_from_fasttext(ft_french, lang=FRENCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-footage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at 1 epoch is 3.27\n",
      "loss at 2 epoch is 2.42\n",
      "loss at 3 epoch is 1.97\n",
      "loss at 4 epoch is 1.63\n",
      "loss at 5 epoch is 1.34\n",
      "loss at 6 epoch is 1.10\n",
      "loss at 7 epoch is 0.91\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "hidden_size = 256\n",
    "vectorizer = dataset.vectorizer\n",
    "english_vocab_size = len(vectorizer.english_vocab)\n",
    "french_vocab_size = len(vectorizer.french_vocab)\n",
    "embedding_size = 100\n",
    "\n",
    "encoder = EncoderRNN(english_vocab_size, hidden_size, embedding_size, english_embedding_matrix)\n",
    "decoder = DecoderRNN(hidden_size, french_vocab_size, embedding_size, french_embedding_matrix)\n",
    "model = SimpleRnn(encoder, decoder)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), 0.001)\n",
    "\n",
    "\n",
    "nb_epochs = 10\n",
    "for epoch in range(nb_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch in generate_batches(dataset, batch_size=1):\n",
    "        single_sentence, translation_sentence = batch[ENCODER_INPUT], batch[DECODER_INPUT]\n",
    "        single_sentence = single_sentence.squeeze(0)\n",
    "        translation_sentence = translation_sentence.squeeze(0)\n",
    "        loss = train_one_sent(\n",
    "            single_sentence, \n",
    "            translation_sentence, \n",
    "            model,\n",
    "            optim\n",
    "        )\n",
    "        epoch_losses.append(loss)\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    print(f'loss at {epoch+1} epoch is {avg_loss:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "heated-recorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il il il il il il il il il\n"
     ]
    }
   ],
   "source": [
    "sentence = \"he likes school\"\n",
    "translate(sentence, model, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-snapshot",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention_env",
   "language": "python",
   "name": "attention_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
