{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sustainable-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "improved-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "\n",
    "from attention import models\n",
    "from attention import utils\n",
    "from attention.vectorizer import Vectorizer\n",
    "from attention.constants import ENGLISH, FRENCH, SEQ_SIZE, DECODER_INPUT, ENCODER_INPUT, SOS_token\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SOURCE_DIR = Path('../')\n",
    "DATA_DIR = SOURCE_DIR / 'data'\n",
    "translation_fp = DATA_DIR / 'eng-fra.txt'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-queue",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "proof-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer code\n",
    "from attention.data import load_sentences_dataframe, assign_rows_to_split, TranslationDataset, generate_batches\n",
    "from attention.vectorizer import Vectorizer\n",
    "\n",
    "df = load_sentences_dataframe(translation_fp)\n",
    "df = assign_rows_to_split(df, train_ratio=0.9, valid_ratio=0.05, test_ratio=0.05)\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "df[ENGLISH] = df[ENGLISH].str.lower()\n",
    "df = df[df[ENGLISH].str.startswith(eng_prefixes)]\n",
    "\n",
    "dataset = TranslationDataset.from_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-salvation",
   "metadata": {},
   "source": [
    "# Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "covered-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model params\n",
    "from attention.models import EncoderRNN, DecoderRNN\n",
    "from attention.embeddings import create_spacy_embeddings\n",
    "\n",
    "hidden_size = 256\n",
    "embedding_size = 300\n",
    "english_vocab_size = len(dataset.vectorizer.english_vocab)\n",
    "french_vocab_size = len(dataset.vectorizer.french_vocab)\n",
    "\n",
    "use_pretrained_embeddings = False\n",
    "english_embedding_matrix, french_embedding_matrix = create_spacy_embeddings(dataset.vectorizer)\n",
    "\n",
    "# use pretrained embeddings?\n",
    "if use_pretrained_embeddings:\n",
    "    encoder = EncoderRNN(english_vocab_size, hidden_size, embedding_matrix=english_embedding_matrix)\n",
    "    decoder = DecoderRNN(hidden_size, french_vocab_size, embedding_matrix=french_embedding_matrix)\n",
    "else:\n",
    "    encoder = EncoderRNN(english_vocab_size, hidden_size, embedding_size)\n",
    "    decoder = DecoderRNN(hidden_size, french_vocab_size, embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-canberra",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "inappropriate-superior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at 1 epoch is 1.08\n",
      "loss at 2 epoch is 0.86\n",
      "loss at 3 epoch is 0.86\n",
      "loss at 4 epoch is 0.79\n",
      "loss at 5 epoch is 0.84\n",
      "loss at 6 epoch is 0.80\n",
      "loss at 7 epoch is 0.79\n",
      "loss at 8 epoch is 0.83\n",
      "loss at 9 epoch is 0.75\n",
      "loss at 10 epoch is 0.75\n",
      "loss at 11 epoch is 0.79\n",
      "loss at 12 epoch is 0.76\n",
      "loss at 13 epoch is 0.75\n",
      "loss at 14 epoch is 0.76\n",
      "loss at 15 epoch is 0.72\n"
     ]
    }
   ],
   "source": [
    "from attention.train import train_simpleRNN_batch\n",
    "\n",
    "# training params\n",
    "nb_epochs = 15\n",
    "learning_rate = 0.001\n",
    "ignore_index = dataset.vectorizer.english_vocab.lookup_token(dataset.vectorizer.english_vocab.pad)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "batch_size = 32\n",
    "\n",
    "# create optimizers\n",
    "encoder_optim = torch.optim.Adam(encoder.parameters(), learning_rate)\n",
    "decoder_optim = torch.optim.Adam(decoder.parameters(), learning_rate)\n",
    "\n",
    "# run epochs\n",
    "for epoch in range(nb_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch in generate_batches(dataset, batch_size=batch_size):\n",
    "        input_batch, target_batch = batch[ENCODER_INPUT], batch[DECODER_INPUT]\n",
    "        loss = train_simpleRNN_batch(\n",
    "            input_batch, \n",
    "            target_batch, \n",
    "            encoder, \n",
    "            decoder, \n",
    "            encoder_optim, \n",
    "            decoder_optim,\n",
    "            criterion,\n",
    "            device\n",
    "        )\n",
    "        epoch_losses.append(loss)\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    print(f'loss at {epoch+1} epoch is {avg_loss:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-fever",
   "metadata": {},
   "source": [
    "# Inference (Translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "moderate-pharmacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am only warming up now. => pour l instant je sejourne a un poisson\n",
      "you are both in the wrong. => il est dans le pays entier .\n",
      "he is said to have died => on dit qu il est mon parent\n",
      "i am bored out of my mind. => je suis a present a l heroine .\n",
      "i am going to stay here for a couple of days. => je suis desolee est ce que je ne le\n",
      "they are out shopping. => elles sont sorties faire des deux\n",
      "i am afraid he will make a mistake. => j ai peur de ce que j ai des\n",
      "we are worried about you. => nous sommes inquiets pour leur securite .\n",
      "he likes to go to work => il travaille dans la recherche sur le\n",
      "he is not at all foolish => il n est pas du tout un\n"
     ]
    }
   ],
   "source": [
    "from attention.utils import translate_simple_rnn\n",
    "\n",
    "sentences = [\n",
    "    \"i am only warming up now.\",\n",
    "    \"you are both in the wrong.\",\n",
    "    \"he is said to have died\",\n",
    "    \"i am bored out of my mind.\",\n",
    "    \"i am going to stay here for a couple of days.\",\n",
    "    \"they are out shopping.\",\n",
    "    \"i am afraid he will make a mistake.\",\n",
    "    \"we are worried about you.\",\n",
    "    \"he likes to go to work\",\n",
    "    \"he is not at all foolish\"\n",
    "]\n",
    "for sent in sentences:\n",
    "    translation = translate_simple_rnn(sent, encoder, decoder, dataset.vectorizer, device)\n",
    "    print(sent, '=>', translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "floral-garlic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32577</th>\n",
       "      <td>he is sure to succeed.</td>\n",
       "      <td>Il est sûr de son succès.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53211</th>\n",
       "      <td>i am bored out of my mind.</td>\n",
       "      <td>Je m'ennuie tellement.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47651</th>\n",
       "      <td>he is not at all foolish.</td>\n",
       "      <td>Il n'est vraiment pas fou.</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100980</th>\n",
       "      <td>we are going to climb that mountain.</td>\n",
       "      <td>Nous allons gravir cette montagne.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51149</th>\n",
       "      <td>we are worried about you.</td>\n",
       "      <td>Nous sommes inquiets à votre sujet.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     english  \\\n",
       "32577                 he is sure to succeed.   \n",
       "53211             i am bored out of my mind.   \n",
       "47651              he is not at all foolish.   \n",
       "100980  we are going to climb that mountain.   \n",
       "51149              we are worried about you.   \n",
       "\n",
       "                                     french  split  \n",
       "32577             Il est sûr de son succès.  train  \n",
       "53211                Je m'ennuie tellement.  train  \n",
       "47651            Il n'est vraiment pas fou.  valid  \n",
       "100980   Nous allons gravir cette montagne.  train  \n",
       "51149   Nous sommes inquiets à votre sujet.  train  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-collect",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention_env",
   "language": "python",
   "name": "attention_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
